{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MirroredStragety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give a simple example of MirroredStragety in Tensorflow which can be used for multi-gpus training.\n",
    "\n",
    "* Tensorflow 1.13.1\n",
    "* Numpy 1.16.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.distribute import values\n",
    "from tensorflow.python.util import nest\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "# from tensorflow.python.util import deprecation\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "# deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "print(tf.VERSION, tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Fake Data\n",
    "\n",
    "* Data point: (x1, x2) \n",
    "* Data label: 1 if x1 > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(bs):\n",
    "    image = np.random.uniform(-1, 1, size=(bs, 2))\n",
    "    label = image[:, 0] > 0\n",
    "    return image.astype(np.float32), label.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network and Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(features, labels):\n",
    "    out = keras.layers.Dense(2, kernel_initializer=\"ones\")(features)\n",
    "    losses = tf.losses.sparse_softmax_cross_entropy(logits=out, labels=labels,\n",
    "                                                    reduction=tf.losses.Reduction.NONE)\n",
    "    # Here we set reduction to NONE for checking a batch of losses in single gpu training\n",
    "    return losses\n",
    "\n",
    "def solver(loss):\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train_op = opt.minimize(loss, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Wrapper, model_fn and per_device_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper(collections.namedtuple('Father', ['train_op', 'losses', 'loss'])):\n",
    "    def __new__(cls, train_op, losses, loss):\n",
    "        return super(Wrapper, cls).__new__(cls, train_op, losses, loss)\n",
    "\n",
    "def model_fn(features, labels):\n",
    "    losses = network(features, labels)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    train_op = solver(loss)\n",
    "    return Wrapper(train_op=train_op, losses=losses, loss=loss)\n",
    "\n",
    "def per_device_dataset(batch, devices):\n",
    "    \"\"\"\n",
    "    batch: [num_gpus, batch_size / num_gpus, data_dim1, data_dim2, ...], here we have shape of [2, 1, 2]\n",
    "    devices: gpu device names\n",
    "    \"\"\"\n",
    "    index = {}\n",
    "\n",
    "    def get_ith(i_):\n",
    "        return lambda x: x[i_]\n",
    "    \n",
    "    for i, d in enumerate(devices):\n",
    "        index[d] = nest.map_structure(get_ith(i), batch)\n",
    "        \n",
    "    return values.regroup(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single GPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_1():\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(1234)\n",
    "    x_input = tf.placeholder(tf.float32, shape=(2, 2), name=\"x_input\")\n",
    "    y_input = tf.placeholder(tf.int32, shape=(2, ), name=\"y_input\")\n",
    "    wp = model_fn(x_input, y_input)\n",
    "\n",
    "    np.random.seed(1234)\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        x, y = gen_data(bs=2)\n",
    "        _, l1, l2 = sess.run([wp.train_op, wp.loss, wp.losses], feed_dict={x_input: x, y_input: y})\n",
    "        if i % 100 == 0:\n",
    "            # Print mean_loss and losses for checking loss = reduce_mean(losses) and comparing with mirrored strategy results\n",
    "            print(\"step {}, loss {} {}\".format(i, l1, l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPUs Training\n",
    "\n",
    "Here we just use 2 gpus.\n",
    "\n",
    "Single GPU training and Double-GPU training shoule produce the same loss and the same weight updates.\n",
    "\n",
    "You can run main_1() and main_2() to check if the results are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_2():\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(1234)\n",
    "    strategy = tf.distribute.MirroredStrategy([\"device:GPU:0\", \"device:GPU:1\"])\n",
    "\n",
    "    with strategy.scope():\n",
    "        x_input = tf.placeholder(tf.float32, shape=(2, 2), name=\"x_input\")\n",
    "        y_input = tf.placeholder(tf.int32, shape=(2, ), name=\"y_input\")\n",
    "\n",
    "        # -----------------------------------------------------------------------------------\n",
    "        # Convert a batch with shape [bs, dim] to a batch [num_gpus, bs/num_gpus, dim]\n",
    "        features, labels = per_device_dataset((tf.reshape(x_input, (2, 1, 2)),\n",
    "                                               tf.reshape(y_input, (2, 1))), strategy.extended._devices)\n",
    "        # Then we get a PerReplica instances, whose each gpu entry will be a batch with shape [bs/num_gpus, dim]\n",
    "        # Try print(features)\n",
    "        # And get PerReplica:{'/replica:0/task:0/device:GPU:0': <tf.Tensor 'strided_slice:0' shape=(1, 2) dtype=float32>, '/replica:0/task:0/device:GPU:1': <tf.Tensor 'strided_slice_2:0' shape=(1, 2) dtype=float32>}\n",
    "        \n",
    "        # Call model_fn for each replica(i.e. gpu)\n",
    "        grouped_wp = strategy.call_for_each_replica(model_fn, args=(features, labels))\n",
    "        # Get loss reduction across all the gpus, i.e. mean loss\n",
    "        mean_loss = strategy.reduce(tf.distribute.get_loss_reduction(), grouped_wp.loss)\n",
    "        # We can also get losses from all the gpus for checking\n",
    "        concat_loss = tf.stack(strategy.unwrap(grouped_wp.loss), axis=0)\n",
    "        # We just need group train_op \n",
    "        train_op = strategy.group(grouped_wp.train_op)\n",
    "        # -----------------------------------------------------------------------------------\n",
    "\n",
    "        np.random.seed(1234)\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(1000):\n",
    "            x, y = gen_data(bs=2)\n",
    "            _, l1, l2 = sess.run([train_op, mean_loss, concat_loss], feed_dict={x_input: x, y_input: y})\n",
    "            if i % 100 == 0:\n",
    "                # Check mean_loss = reduce_mean(concat_loss)\n",
    "                print(\"step {}, loss {} {}\".format(i, l1, l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1.13] *",
   "language": "python",
   "name": "conda-env-tf1.13-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
